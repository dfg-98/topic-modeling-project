{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Launch the CoherenceTestIwor.py program with default dataset used in the Python code that is TokenVieuxM.txt \n",
    "\n",
    "* a) Launch the program several times. What did you observe regarding the obtained coherence and perplexity values ? Try to provide a reason for your observation (looking to papers or Web infos referring to LDA topic identification process). Consequently, explain how it can be possible to obtain the best results regarding coherence and perplexity.\n",
    "\n",
    "* b) Look the the topic descriptions obtained with one specific launch. Present the topic description output you obtain along with coherence and perplexity values. What can you conclude regarding these descriptions and theses valuesÂ ? Explain if you consider that the results are good are bad. Justify your answer in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dario/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dependecies\n",
    "import nltk\n",
    "# Create a corpus from a list of lists of tokens\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "file = open(\"TokenVieuxM.txt\", \"r\")\n",
    "lines = file.readlines()\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "for line in lines:\n",
    "    line=line.strip()\n",
    "    lt=line.split(\",\")\n",
    "    #Potential ill-character cleaning\n",
    "    for i in range(len(lt)):\n",
    "        lt[i]=lt[i].replace('[','')\n",
    "        lt[i]=lt[i].replace(']','')\n",
    "        lt[i]=lt[i].replace('\"','')\n",
    "        lt[i]=lt[i].replace('\\n','')\n",
    "        lt[i]=lt[i].replace(' ', '')\n",
    "    #End : Potential ill-characters cleaning\n",
    "    # print(lt)\n",
    "    # ltc=[word for word in lt if not word in stopwords.words()]\n",
    "    # print(\"C\", ltc)\n",
    "    texts.append(lt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here set the number of topics(to be changed if necessary)\n",
    "nb=10\n",
    "  \n",
    "id2word = Dictionary(texts)\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "#print(corpus)\n",
    "\n",
    "# # Print dictionnary\n",
    "# for i in id2word:\n",
    "#   print(i, id2word[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the lda model on the corpus.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "lda = LdaModel(corpus, num_topics=nb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0\n",
      "of  - P= 0.032965027\n",
      "and  - P= 0.032303113\n",
      "to  - P= 0.030246342\n",
      "the  - P= 0.029939067\n",
      "in  - P= 0.021617722\n",
      "with  - P= 0.014988868\n",
      "their  - P= 0.014757316\n",
      "structural  - P= 0.013033258\n",
      "these  - P= 0.0125924\n",
      "strategies  - P= 0.011675524\n",
      "\n",
      "Topic  1\n",
      "the  - P= 0.029451385\n",
      "and  - P= 0.026933555\n",
      "of  - P= 0.019172484\n",
      "auditory  - P= 0.017929075\n",
      "in  - P= 0.012719797\n",
      "neurons  - P= 0.010169702\n",
      "to  - P= 0.009143855\n",
      "are  - P= 0.008719574\n",
      "various  - P= 0.0070741978\n",
      "audiomotor  - P= 0.0058763027\n",
      "\n",
      "Topic  2\n",
      "and  - P= 0.012832806\n",
      "the  - P= 0.010964359\n",
      "of  - P= 0.010058744\n",
      "to  - P= 0.005431109\n",
      "in  - P= 0.0052027926\n",
      "auditory  - P= 0.0051140897\n",
      "neurons  - P= 0.004636731\n",
      "are  - P= 0.0046162033\n",
      "biological  - P= 0.00418508\n",
      "for  - P= 0.0039869635\n",
      "\n",
      "Topic  3\n",
      "of  - P= 0.053148005\n",
      "the  - P= 0.049691465\n",
      "and  - P= 0.039368074\n",
      "to  - P= 0.026892057\n",
      "in  - P= 0.023397615\n",
      "auditory  - P= 0.010968063\n",
      "a  - P= 0.010241951\n",
      "for  - P= 0.008651338\n",
      "are  - P= 0.008533108\n",
      "neurons  - P= 0.007932067\n",
      "\n",
      "Topic  4\n",
      "the  - P= 0.033451155\n",
      "of  - P= 0.02910495\n",
      "and  - P= 0.02005038\n",
      "in  - P= 0.018684551\n",
      "to  - P= 0.015061372\n",
      "is  - P= 0.013385454\n",
      "a  - P= 0.013008702\n",
      "dna  - P= 0.009923056\n",
      "for  - P= 0.009592204\n",
      "protein  - P= 0.006880271\n",
      "\n",
      "Topic  5\n",
      "the  - P= 0.0442129\n",
      "of  - P= 0.041919287\n",
      "and  - P= 0.03671037\n",
      "to  - P= 0.02620136\n",
      "in  - P= 0.019395126\n",
      "a  - P= 0.0123621505\n",
      "for  - P= 0.011503827\n",
      "with  - P= 0.010828555\n",
      "these  - P= 0.009183\n",
      "their  - P= 0.008281142\n",
      "\n",
      "Topic  6\n",
      "the  - P= 0.0055934703\n",
      "and  - P= 0.004174015\n",
      "of  - P= 0.0038598583\n",
      "auditory  - P= 0.0037355025\n",
      "neurons  - P= 0.0033153915\n",
      "in  - P= 0.0031781977\n",
      "to  - P= 0.0029658705\n",
      "are  - P= 0.0028730058\n",
      "tegmental  - P= 0.0027227106\n",
      "neural  - P= 0.0026383682\n",
      "\n",
      "Topic  7\n",
      "of  - P= 0.031245438\n",
      "the  - P= 0.017183404\n",
      "and  - P= 0.012386005\n",
      "a  - P= 0.011583901\n",
      "to  - P= 0.009960444\n",
      "for  - P= 0.009094964\n",
      "in  - P= 0.008264956\n",
      "is  - P= 0.0077676964\n",
      "biological  - P= 0.0055842046\n",
      "measures  - P= 0.005572448\n",
      "\n",
      "Topic  8\n",
      "the  - P= 0.033725027\n",
      "and  - P= 0.029690143\n",
      "of  - P= 0.029372983\n",
      "to  - P= 0.02673219\n",
      "in  - P= 0.013362736\n",
      "a  - P= 0.01207676\n",
      "for  - P= 0.011618799\n",
      "with  - P= 0.011119163\n",
      "men  - P= 0.0074310363\n",
      "their  - P= 0.0073939515\n",
      "\n",
      "Topic  9\n",
      "the  - P= 0.04147013\n",
      "and  - P= 0.038861867\n",
      "of  - P= 0.025916178\n",
      "in  - P= 0.021824993\n",
      "to  - P= 0.017896071\n",
      "neurons  - P= 0.009737985\n",
      "auditory  - P= 0.008588268\n",
      "are  - P= 0.007753645\n",
      "for  - P= 0.007594721\n",
      "with  - P= 0.0065081622\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print topic descrition\n",
    "for i in range(0, nb):\n",
    "  value=lda.get_topic_terms(i)\n",
    "#  print(value)\n",
    "  print(\"Topic \", i)\n",
    "  for j in value:\n",
    "    print(id2word[j[0]], \" - P=\", j[1])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity=  -7.526049896583079\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "perplexity_lda=lda.log_perplexity(corpus)  # a measure of how good the model is (lower the better).\n",
    "print('Perplexity= ', perplexity_lda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence=  0.33459597719427553\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence= ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_compute_metrics(nb):\n",
    "\n",
    "    # Train the lda model on the corpus.\n",
    "    from gensim.models import LdaModel\n",
    "\n",
    "    lda = LdaModel(corpus, num_topics=nb)\n",
    "\n",
    "    # Print topic descrition\n",
    "    # for i in range(0, nb-1):\n",
    "    #     value=lda.get_topic_terms(i)\n",
    "    #     #  print(value)\n",
    "    #     print(\"Topic \", i)\n",
    "    #     for j in value:\n",
    "    #         print(id2word[j[0]], \" - P=\", j[1])\n",
    "    #     print()\n",
    "\n",
    "    # Compute Perplexity\n",
    "    perplexity_lda=lda.log_perplexity(corpus)  # a measure of how good the model is (lower the better).\n",
    "    # print('Perplexity= ', perplexity_lda)\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    from gensim.models import CoherenceModel\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=lda, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    # print('Coherence= ', coherence_lda)\n",
    "    return perplexity_lda, coherence_lda\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.191772159795034, -7.522204059721802, -7.437199070320737, -7.314576423751345, -7.199724123000556, -7.442886027595424, -7.353710299262414, -7.48433370455947, -6.941227889237279, -7.187834788236738, -7.438033253734294, -7.52544761131241, -7.544524989233896, -7.009913421938443, -7.702930060534212, -7.032278449190897, -7.140420978115416, -7.270083453332469, -7.471877117409234, -7.638414137599411, -7.189738856105132, -7.025425739255781, -7.3365646965538955, -7.414219959468429, -7.410984829627613, -7.232060893675031, -7.179793820378452, -7.2741931242205045, -7.0243961364200365, -7.544641575240981, -7.629902277044333, -7.111960665832472, -7.409063697268125, -7.318910627031489, -7.358435815172662, -7.313154924783826, -7.466702046044451, -7.169366885370769, -7.411112549972208, -7.408851414545946, -7.436131601786586, -7.559518204207307, -7.306268895716662, -6.986136560282745, -7.303805437104287, -7.511955677310219, -7.460578292574357, -7.61724541184574, -7.156611029944457, -7.189920338347504]\n",
      "[0.36292745494255463, 0.3790785675320513, 0.29419163184792313, 0.409563008106734, 0.43543819554145824, 0.33850648389251115, 0.3619223681192044, 0.34454438399417775, 0.45106174024942336, 0.4364022273730515, 0.38169905543850285, 0.32268859995309007, 0.39491741174023265, 0.4727402886042394, 0.3790691537264687, 0.43347836675100615, 0.4207910534380802, 0.45765236531648784, 0.36863349709339366, 0.3237318228550029, 0.456996185072896, 0.4053964101811635, 0.3568453789193028, 0.4594481061131418, 0.33684820291146955, 0.45380701826963843, 0.398637603796285, 0.35652701197569714, 0.4802758872561606, 0.41686521568675455, 0.3615321126128183, 0.4715375320261416, 0.3503108411645843, 0.36597308895176855, 0.4081607148740128, 0.3512391988618649, 0.3129584283735385, 0.3819932505377505, 0.3428796279026149, 0.36029374259852426, 0.34410665645915545, 0.3974927460492553, 0.43026712149372737, 0.44874080189261906, 0.293095324769671, 0.31376879160891524, 0.37743843964110746, 0.40635781072852895, 0.4148697169816682, 0.4573518149756549]\n",
      "Min perplexity: -7.702930060534212\n",
      "Max perplexity: -6.941227889237279\n",
      "Average Perplexity: -7.33214088002027\n",
      "Min coherence: 0.293095324769671\n",
      "Max coherence: 0.4802758872561606\n",
      "Average Coherence:  0.3896210491840405\n"
     ]
    }
   ],
   "source": [
    "perplexities = []\n",
    "coherences = []\n",
    "for i in range(50):\n",
    "    p, c = train_model_and_compute_metrics(10)\n",
    "    perplexities.append(p)\n",
    "    coherences.append(c)\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "print(perplexities)\n",
    "print(coherences)\n",
    "print(f\"Min perplexity: {min(perplexities)}\")\n",
    "print(f\"Max perplexity: {max(perplexities)}\")\n",
    "print(f\"Average Perplexity: {mean(perplexities)}\")\n",
    "print(f\"Min coherence: {min(coherences)}\")\n",
    "print(f\"Max coherence: {max(coherences)}\")\n",
    "print(f\"Average Coherence:  {mean(coherences)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-a)\n",
    "\n",
    "We run the program 50 times, here are some observations:\n",
    "\n",
    "Perplexity:\n",
    "- The minimum perplexity score obtained was -7.702930060534212, while the maximum was -6.941227889237279. The range between the minimum and maximum values suggests that there is some variation in the model's ability to predict the held-out data.\n",
    "- The average perplexity score across all runs was -7.33214088002027. This average value can serve as a baseline for comparing the perplexity of future runs or different LDA configurations.\n",
    "\n",
    "Coherence:\n",
    "- The coherence scores range from 0.293095324769671 to 0.4802758872561606, indicating a noticeable variation in the interpretability and semantic consistency of the generated topics across the different runs.\n",
    "- The average coherence score across all runs was 0.3896210491840405. This average value can serve as a baseline for comparing the coherence of future runs or different LDA configurations.\n",
    "\n",
    "Overall, it appears that there is some variability in the performance of the LDA model across different runs. Some runs yield higher coherence scores, indicating more interpretable and coherent topics, while others have lower coherence scores. Similarly, there is some variation in the perplexity scores, suggesting differences in the model's ability to predict the held-out data.\n",
    "\n",
    "The observed variation in coherence and perplexity scores can be attributed to several factors related to the LDA topic identification process. Here are some reasons for the observed observations:\n",
    "\n",
    "\n",
    "1. Topic Overlap and Ambiguity: In the field of healthcare research, where the corpus consists of abstracts from research papers, there might be inherent topic overlap and ambiguity. Some topics may share common terms or concepts, making it challenging for the model to accurately distinguish between them. This can impact the coherence and perplexity scores, as the model struggles to assign tokens to distinct topics.\n",
    "\n",
    "2. Variability in Data Quality: The quality and diversity of the research papers' abstracts can also influence the LDA model's performance. If the dataset contains papers with varying levels of quality, inconsistency, or noise, it can affect the coherence and perplexity scores. The presence of irrelevant or poorly written abstracts can introduce noise and decrease the model's ability to identify coherent topics.\n",
    "\n",
    "3. Random Initialization: LDA models use random initialization, which means that each run can start from a different initial state. Randomness can lead to different outcomes for each run, resulting in variations in the coherence and perplexity scores. It's possible that some runs initialized with favorable initial conditions led to better topic quality and lower perplexity, while others had less favorable initial states.\n",
    "\n",
    "\n",
    "To obtain the best results in terms of coherence and perplexity for an LDA model, we can consider the following strategies:\n",
    "\n",
    "1. Hyperparameter Tuning: Experiment with different hyperparameter configurations to find the optimal settings for the dataset. This includes adjusting the number of topics, alpha, and beta values. We can try running the model with various combinations of these parameters and select the configuration that yields the highest coherence and lowest perplexity scores. Techniques like grid search or Bayesian optimization can be helpful in automating this process.\n",
    "\n",
    "2. Quality Preprocessing: Ensuring that data preprocessing steps are robust. This includes removing irrelevant or noisy data, such as stopwords, punctuation, and rare terms. Applying stemming or lemmatization techniques can also help reduce the dimensionality of the corpus and improve the quality of the topics generated by the model.\n",
    "\n",
    "3. Text Cleaning: Perform thorough text cleaning to eliminate any inconsistencies, formatting issues, or special characters that might affect the coherence and perplexity scores. Standardizing the text, handling missing values, and resolving any encoding problems are crucial steps to ensure the input data is clean and consistent.\n",
    "\n",
    "4. Dataset Size and Diversity: Consider the size and diversity of the dataset. Larger and more diverse datasets tend to yield better results. If the dataset is small, may be wanted to augment it or consider techniques like cross-validation to assess the stability and generalizability of the generated topics.\n",
    "\n",
    "5. Iterations and Convergence: Increasing the number of iterations during the training process. More iterations allow the model to converge to a more stable state and potentially improve the coherence and perplexity scores. However, it should be take in account the computational resources required for longer training times.\n",
    "\n",
    "6. Evaluation Metrics: While coherence and perplexity are commonly used evaluation metrics, they may not always align with human judgment. It's important to complement these metrics with manual inspection and validation of the generated topics. Human evaluation can provide valuable insights into the interpretability and relevance of the topics.\n",
    "\n",
    "7. Ensembling or Averaging: Instead of relying on a single run, is possible to perform ensembling or averaging of multiple LDA models. This involves running the LDA model multiple times with different random initializations and combining the results to get a more robust representation of the topics. Ensembling or averaging can help mitigate the variability observed in coherence and perplexity scores across individual runs.\n",
    "\n",
    "The topic modeling is an iterative process, and it may require several iterations and experimentation to achieve the best results. It's important to strike a balance between model complexity, computational resources, and the interpretability of the generated topics based on the specific requirements of the domain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic  0\n",
      "and  - P= 0.04079648\n",
      "the  - P= 0.03669876\n",
      "of  - P= 0.035698477\n",
      "to  - P= 0.020153081\n",
      "with  - P= 0.015370513\n",
      "in  - P= 0.014031986\n",
      "their  - P= 0.011863557\n",
      "are  - P= 0.008931493\n",
      "for  - P= 0.008719349\n",
      "strategies  - P= 0.0074828193\n",
      "\n",
      "Topic  1\n",
      "the  - P= 0.042128753\n",
      "and  - P= 0.037088867\n",
      "of  - P= 0.036617726\n",
      "to  - P= 0.020170096\n",
      "in  - P= 0.01629544\n",
      "auditory  - P= 0.015384982\n",
      "neurons  - P= 0.011424743\n",
      "are  - P= 0.0081757605\n",
      "for  - P= 0.0075630527\n",
      "with  - P= 0.007406899\n",
      "\n",
      "Topic  2\n",
      "the  - P= 0.041734356\n",
      "of  - P= 0.041273266\n",
      "and  - P= 0.03333703\n",
      "in  - P= 0.028039215\n",
      "to  - P= 0.027878301\n",
      "a  - P= 0.011835274\n",
      "that  - P= 0.0095071765\n",
      "for  - P= 0.009258195\n",
      "we  - P= 0.00742808\n",
      "are  - P= 0.0073413337\n",
      "\n",
      "Topic  3\n",
      "the  - P= 0.032281876\n",
      "of  - P= 0.027554318\n",
      "a  - P= 0.027267015\n",
      "to  - P= 0.02571993\n",
      "in  - P= 0.017915018\n",
      "is  - P= 0.017450055\n",
      "for  - P= 0.014408661\n",
      "and  - P= 0.012528666\n",
      "protein  - P= 0.011475687\n",
      "proteomics  - P= 0.010590326\n",
      "\n",
      "Topic  4\n",
      "and  - P= 0.044136\n",
      "of  - P= 0.039803825\n",
      "the  - P= 0.036199786\n",
      "to  - P= 0.027420614\n",
      "in  - P= 0.014018859\n",
      "with  - P= 0.013234748\n",
      "their  - P= 0.012540054\n",
      "for  - P= 0.009772961\n",
      "are  - P= 0.009356252\n",
      "structural  - P= 0.00917114\n",
      "\n",
      "Topic  5\n",
      "the  - P= 0.044788674\n",
      "of  - P= 0.03935857\n",
      "in  - P= 0.0293257\n",
      "and  - P= 0.025861379\n",
      "to  - P= 0.01640411\n",
      "a  - P= 0.011435788\n",
      "dna  - P= 0.010612665\n",
      "is  - P= 0.010577212\n",
      "that  - P= 0.00849989\n",
      "ice  - P= 0.007870118\n",
      "\n",
      "Topic  6\n",
      "the  - P= 0.033261422\n",
      "to  - P= 0.022312237\n",
      "of  - P= 0.020712523\n",
      "and  - P= 0.018019073\n",
      "in  - P= 0.012606089\n",
      "with  - P= 0.00828004\n",
      "auditory  - P= 0.00782432\n",
      "their  - P= 0.007682049\n",
      "for  - P= 0.0075910473\n",
      "structural  - P= 0.0073817647\n",
      "\n",
      "Topic  7\n",
      "of  - P= 0.019411828\n",
      "the  - P= 0.012528531\n",
      "and  - P= 0.01235252\n",
      "in  - P= 0.011682552\n",
      "to  - P= 0.011649223\n",
      "with  - P= 0.0064717582\n",
      "a  - P= 0.0050964816\n",
      "these  - P= 0.0050648255\n",
      "we  - P= 0.005035239\n",
      "their  - P= 0.0048551424\n",
      "\n",
      "Topic  8\n",
      "the  - P= 0.026131216\n",
      "and  - P= 0.021618005\n",
      "of  - P= 0.018261962\n",
      "to  - P= 0.012243422\n",
      "in  - P= 0.012023876\n",
      "for  - P= 0.00845984\n",
      "with  - P= 0.007344131\n",
      "auditory  - P= 0.0068506417\n",
      "their  - P= 0.0062986724\n",
      "we  - P= 0.0062296083\n",
      "\n",
      "Topic  9\n",
      "of  - P= 0.04099069\n",
      "and  - P= 0.03784173\n",
      "the  - P= 0.030940356\n",
      "to  - P= 0.019591307\n",
      "in  - P= 0.010788923\n",
      "a  - P= 0.009998699\n",
      "biological  - P= 0.009782253\n",
      "for  - P= 0.008921841\n",
      "on  - P= 0.007939334\n",
      "are  - P= 0.0068005053\n",
      "\n",
      "Perplexity=  -7.622905512176141\n",
      "Coherence=  0.35474875290840896\n"
     ]
    }
   ],
   "source": [
    "nb = 10\n",
    "\n",
    "# Train the lda model on the corpus.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "lda = LdaModel(corpus, num_topics=nb)\n",
    "\n",
    "# Print topic descrition\n",
    "for i in range(0, nb):\n",
    "  value=lda.get_topic_terms(i)\n",
    "#  print(value)\n",
    "  print(\"Topic \", i)\n",
    "  for j in value:\n",
    "    print(id2word[j[0]], \" - P=\", j[1])\n",
    "  print()\n",
    "\n",
    "# Compute Perplexity\n",
    "perplexity_lda=lda.log_perplexity(corpus)  # a measure of how good the model is (lower the better).\n",
    "print('Perplexity= ', perplexity_lda)\n",
    "\n",
    "# Compute Coherence Score\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence= ', coherence_lda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- b)\n",
    "\n",
    "```\n",
    "Topic  0\n",
    "of  - P= 0.011058775\n",
    "the  - P= 0.009887789\n",
    "in  - P= 0.009401254\n",
    "to  - P= 0.006715426\n",
    "and  - P= 0.0062425523\n",
    "ice  - P= 0.0060687326\n",
    "that  - P= 0.005530666\n",
    "evidence  - P= 0.0053881146\n",
    "months  - P= 0.005122016\n",
    "age  - P= 0.004753692\n",
    "\n",
    "Topic  1\n",
    "and  - P= 0.003016283\n",
    "the  - P= 0.0027025968\n",
    "of  - P= 0.0026951975\n",
    "neurons  - P= 0.0024320884\n",
    "auditory  - P= 0.0024301207\n",
    "to  - P= 0.0024213113\n",
    "in  - P= 0.0023258585\n",
    "for  - P= 0.0023029167\n",
    "are  - P= 0.0022783382\n",
    "has  - P= 0.0022629541\n",
    "\n",
    "Topic  2\n",
    "of  - P= 0.04065094\n",
    "and  - P= 0.03873536\n",
    "the  - P= 0.03197248\n",
    "to  - P= 0.025909802\n",
    "in  - P= 0.016355481\n",
    "a  - P= 0.015465702\n",
    "for  - P= 0.015459485\n",
    "on  - P= 0.010814473\n",
    "is  - P= 0.008926802\n",
    "biological  - P= 0.0070602344\n",
    "\n",
    "Topic  3\n",
    "of  - P= 0.04618509\n",
    "the  - P= 0.04207941\n",
    "and  - P= 0.03924803\n",
    "to  - P= 0.02932435\n",
    "in  - P= 0.020202585\n",
    "with  - P= 0.016553894\n",
    "these  - P= 0.013052856\n",
    "their  - P= 0.012730474\n",
    "strategies  - P= 0.011087511\n",
    "men  - P= 0.011045343\n",
    "\n",
    "Topic  4\n",
    "the  - P= 0.04434207\n",
    "of  - P= 0.036716737\n",
    "and  - P= 0.024621978\n",
    "to  - P= 0.024028737\n",
    "a  - P= 0.023211565\n",
    "in  - P= 0.018962674\n",
    "is  - P= 0.013509136\n",
    "for  - P= 0.010867239\n",
    "protein  - P= 0.00973452\n",
    "dna  - P= 0.009628494\n",
    "\n",
    "Topic  5\n",
    "of  - P= 0.032803472\n",
    "the  - P= 0.02763419\n",
    "and  - P= 0.023926117\n",
    "in  - P= 0.022081278\n",
    "to  - P= 0.01887759\n",
    "for  - P= 0.007857354\n",
    "a  - P= 0.007245258\n",
    "that  - P= 0.0072033205\n",
    "months  - P= 0.0069121956\n",
    "are  - P= 0.0063646236\n",
    "\n",
    "Topic  6\n",
    "and  - P= 0.0462492\n",
    "the  - P= 0.039982613\n",
    "of  - P= 0.030670736\n",
    "in  - P= 0.024383793\n",
    "to  - P= 0.019571112\n",
    "neurons  - P= 0.013130104\n",
    "auditory  - P= 0.0111098355\n",
    "for  - P= 0.007851063\n",
    "are  - P= 0.0076545\n",
    "that  - P= 0.007559434\n",
    "\n",
    "Topic  7\n",
    "the  - P= 0.05403955\n",
    "of  - P= 0.026726577\n",
    "and  - P= 0.025404995\n",
    "auditory  - P= 0.023493377\n",
    "to  - P= 0.017092828\n",
    "in  - P= 0.013509813\n",
    "neurons  - P= 0.011250108\n",
    "are  - P= 0.010783905\n",
    "for  - P= 0.0077176117\n",
    "be  - P= 0.007553489\n",
    "\n",
    "Topic  8\n",
    "of  - P= 0.03431679\n",
    "the  - P= 0.032735877\n",
    "and  - P= 0.027877424\n",
    "in  - P= 0.0225479\n",
    "to  - P= 0.01929094\n",
    "that  - P= 0.009479902\n",
    "ice  - P= 0.009157433\n",
    "a  - P= 0.008141336\n",
    "dna  - P= 0.006896983\n",
    "neurons  - P= 0.006812114\n",
    "\n",
    "Topic  9\n",
    "of  - P= 0.022648524\n",
    "and  - P= 0.018146789\n",
    "the  - P= 0.017388334\n",
    "to  - P= 0.01587528\n",
    "with  - P= 0.009157343\n",
    "their  - P= 0.00729402\n",
    "for  - P= 0.007259891\n",
    "are  - P= 0.0066940407\n",
    "structural  - P= 0.0066021215\n",
    "in  - P= 0.0059986953\n",
    "\n",
    "Perplexity=  -7.244668698758395\n",
    "Coherence=  0.39542486916238984\n",
    "```\n",
    "\n",
    "Based on the topics and the corresponding perplexity and coherence values, we can analyze the results as follows:\n",
    "\n",
    "1. Topic Descriptions: The topic descriptions consist of the top words and their corresponding probabilities for each topic. From a qualitative perspective, it is difficult to determine the specific theme or subject of each topic based solely on the top words. The words appear to be common and generic, such as \"of,\" \"the,\" \"and,\" \"to,\" etc. However, there are some topic-specific terms like \"ice,\" \"neurons,\" \"auditory,\" \"protein,\" \"DNA,\" etc., which suggest potential themes related to neuroscience, genetics, and biological research.\n",
    "\n",
    "2. Perplexity: The perplexity value indicates how well the LDA model predicts the held-out test data. A lower perplexity score generally suggests better model performance. In this case, the perplexity value of -7.2446 is lower than the average perplexity (-7.3321) obtained from the 50 runs. This indicates that the model performs relatively well in terms of predicting unseen data, suggesting that the model has learned the underlying patterns of the corpus to some extent.\n",
    "\n",
    "3. Coherence: The coherence value measures the degree of semantic similarity among the top words within each topic. A higher coherence score indicates that the topics are more coherent and interpretable. The coherence score of 0.3954 is higher than the average coherence (0.3896) obtained from the 50 runs. This suggests that the generated topics have a reasonable degree of semantic coherence.\n",
    "\n",
    "Considering these observations, we can conclude the following:\n",
    "\n",
    "a) Results Assessment: Based on the perplexity and coherence values, the obtained results can be considered relatively good. The perplexity score is lower than the average, indicating that the model performs well in predicting unseen data. The coherence score is higher than the average, suggesting that the topics exhibit a reasonable level of semantic coherence.\n",
    "\n",
    "b) Interpretability: However, when examining the topic descriptions, the top words for each topic appear to be quite generic, including common words like \"of,\" \"the,\" and \"and,\" which do not provide clear thematic information. This indicates that the model may not have captured specific and distinct topics within the healthcare research domain. Removing stopwords could highly improve the results.\n",
    "\n",
    "c) Domain Relevance: It is important to assess the relevance and interpretability of the topics within the specific context of healthcare research. The generic nature of the top words raises questions about the specificity and meaningfulness of the generated topics. Further evaluation and validation from domain experts are necessary to determine the relevance and usefulness of the identified topics for healthcare research.\n",
    "\n",
    "d) Comparison to Other Runs: To make a more comprehensive judgment, it would be valuable to compare these results with the average perplexity and coherence values obtained from the full set of 50 runs. Additionally, visualizing the topics using techniques like word clouds or examining topic-document distributions could provide additional insights into the quality and meaningfulness of the topics.\n",
    "\n",
    "In summary, while the perplexity and coherence scores suggest relatively good model performance, the interpretability and specificity of the generated topics need further evaluation. Additional iterations, hyperparameter tuning, or incorporating domain-specific knowledge may be necessary to improve the results and generate more meaningful and relevant topics for healthcare research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
