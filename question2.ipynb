{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Modify the CoherenceTestIwor.py program to remove stopwords from document descriptions.\n",
    "\n",
    "a) Explain what are representing stopwords exactly (looking to papers or Web infos on that topic).\n",
    "\n",
    "b) Go back to the results of question 1) b) and considering your response to question 2) a) explain what is the main content of topics obtained in question 1) b). Try to explain why.\n",
    "\n",
    "c) For removing the stopwords from documents descriptions, you will need to modify the CoherenceTestIwor.py, as it is mentioned on 2). For that purpose, you can make use of some commented instructions in the program alongside with some slight modifications of some other intructions of the program. Describe which instructions you have modified and why.\n",
    " \n",
    "Relaunch your modified program and present your obtained results including topic decriptions, coherence and perplexity values. Compare the results with the ones of question 1) b). Consequently, estimate the relationship between coherence, perplexity and topic quality.\n",
    "\n",
    "For the next questions you will have to use your modified program including the stopwords removal function.\n",
    "\n",
    "d) Launch the modified program several times making varying the number of expected topics from 5 to 20 and use the obtained values of coherence for each different number of topics to draw a chart. Looking to the obtained chart, try to conclude what should be the optimal number of topics for the document collection TokenVieuxM.txt. Justify your answer.\n",
    "\n",
    "e) Change the dataset used by your modified program including the stop words removal function in order to use TokenVieuxN.txt document file instead of TokenVieuxM.txt document file. Use an expected number of topics equal to 10 to compare the results of this experiment with the experiment achieved with TokenVieuxM.txt in 2) c). What can you conclude ?\n",
    "\n",
    "f) Make the same experience as the one achieved in 2) d) but using TokenVieuxN.txt instead of  TokenVieuxM.txt and  number of expected topics varying from 5 to 30. Looking to the new obtained chart (that should be presented), draw a new conclusion.\n",
    "\n",
    "Can you conclude that the optimal number of topic depend on the exploited dataset ? Observe the content of the two datasets TokenVieuxM.txt and TokenVieuxN.txt more carefully and try to find the differences between the two. What can you conclude regarding the dependance between the optimal number of topic and the charateristics of the datasets after your observations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- a)\n",
    "\n",
    "Stopwords in the context of topic modeling, such as in LDA (Latent Dirichlet Allocation), refer to commonly occurring words in a language that are considered to have little or no semantic meaning or do not contribute significantly to the understanding or differentiation of documents. These words include articles (e.g., \"the,\" \"a\"), prepositions (e.g., \"of,\" \"in\"), conjunctions (e.g., \"and,\" \"or\"), pronouns (e.g., \"he,\" \"she\"), and other frequently used words.\n",
    "\n",
    "The use of stopwords in LDA topic modeling is based on the assumption that these words occur frequently across different documents and topics, and their inclusion in the analysis may introduce noise and hinder the identification of more meaningful and distinctive topics. By removing stopwords, the focus can be shifted towards identifying and extracting more informative and topic-specific terms.\n",
    "\n",
    "Stopwords are typically removed during the preprocessing stage of topic modeling. The preprocessing steps often involve tokenization (splitting text into individual words or tokens), lowercasing, and the removal of punctuation, special characters, and stopwords. This process helps to clean the text data and prepare it for further analysis, such as building the LDA model.\n",
    "\n",
    "The specific list of stopwords used may vary depending on the language and the specific task. Commonly used stopwords are often based on general language usage, but domain-specific stopwords can also be added to remove domain-specific noise. For example, in the context of healthcare research, domain-specific stopwords could include terms like \"patient,\" \"study,\" or \"treatment,\" which may be frequently occurring but may not contribute much to the differentiation of topics in a healthcare research corpus.\n",
    "\n",
    "Overall, the removal of stopwords in LDA topic modeling aims to improve the quality and interpretability of the generated topics by focusing on more meaningful and topic-specific terms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- b)\n",
    "\n",
    "Since the corpus was not cleaned from stopwords, the presence of these common words might have influenced the topics obtained.\n",
    "\n",
    "Looking at the obtained topics:\n",
    "\n",
    "Topic 0:\n",
    "This topic seems to contain common words such as \"of,\" \"the,\" \"in,\" \"to,\" and \"and.\" These words are frequently occurring stopwords that do not provide much specific information about the topic. It suggests that this topic may not have a clear and distinctive theme and could be a mixture of different subjects.\n",
    "\n",
    "Topic 1:\n",
    "Similar to the previous topic, this topic also contains stopwords like \"and,\" \"the,\" \"of,\" and \"to.\" Additionally, it includes terms such as \"neurons\" and \"auditory,\" which might indicate a connection to neuroscience or auditory-related research. However, without further context, it is difficult to determine the exact theme of this topic.\n",
    "\n",
    "Topic 2:\n",
    "Again, we observe the presence of common stopwords like \"of,\" \"the,\" \"and,\" \"in,\" and \"to.\" The inclusion of the term \"biological\" suggests that this topic might be related to biological aspects in healthcare research. However, the specific focus or subtopic within the broader field of healthcare is not clear from this information alone.\n",
    "\n",
    "Topic 3:\n",
    "This topic includes terms like \"strategies\" and \"men\" along with stopwords such as \"of,\" \"the,\" and \"and.\" It is challenging to discern the main content or theme solely based on these terms. Further information or analysis is required to determine the specific subject matter of this topic.\n",
    "\n",
    "Topic 4:\n",
    "Once again, we see common stopwords like \"the,\" \"of,\" \"and,\" \"to,\" and \"in.\" The inclusion of terms like \"protein\" and \"DNA\" suggests that this topic might be related to molecular biology or genetic research within the healthcare domain. However, additional context is needed to understand the precise focus of this topic.\n",
    "\n",
    "Topics 5, 6, 7, 8, and 9:\n",
    "These topics also exhibit the presence of common stopwords without clear and specific terms that could indicate a distinct theme. Without further context or analysis, it is challenging to determine the main content of these topics.\n",
    "\n",
    "Overall, the presence of stopwords in the corpus might have diluted the topics obtained and made it difficult to identify specific themes or subject matters. Stopwords, being common and frequently occurring words, do not contribute much to the differentiation and specificity of topics. To obtain more meaningful and interpretable topics, it is advisable to preprocess the corpus by removing stopwords before running the LDA model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- c)\n",
    "\n",
    "For removing stopwords just add the line:\n",
    "```python\n",
    "ltc=[word for word in lt if not word in stopwords.words()]\n",
    "```\n",
    "and replace:\n",
    "```python\n",
    "texts.append(lt)\n",
    "```\n",
    "by \n",
    "```python\n",
    "texts.append(ltc)\n",
    "```\n",
    "inside the for structure for cleaning data (See [CoherenceTestIworWithoutStopWords.py](https://github.com/dfg-98/topic-modeling-project/blob/master/CoherenceTestIworWithoutStopWords.py))\n",
    "\n",
    "Running the program without stopwords we get the following output:\n",
    "\n",
    "```\n",
    "Topic  0\n",
    "structural  - P= 0.018502709\n",
    "strategies  - P= 0.018321123\n",
    "husbands  - P= 0.01709903\n",
    "work  - P= 0.013490833\n",
    "experiences  - P= 0.012242352\n",
    "deal  - P= 0.010832099\n",
    "approaches  - P= 0.009287285\n",
    "tie  - P= 0.009140642\n",
    "caregiving  - P= 0.008932805\n",
    "gender  - P= 0.008596426\n",
    "\n",
    "Topic  1\n",
    "dna  - P= 0.0130954655\n",
    "protein  - P= 0.012843002\n",
    "proteomics  - P= 0.012238255\n",
    "proteins  - P= 0.010161503\n",
    "analysis  - P= 0.008931611\n",
    "modifications  - P= 0.008900921\n",
    "biological  - P= 0.008656172\n",
    "experiences  - P= 0.008406849\n",
    "mental  - P= 0.008315007\n",
    "strategies  - P= 0.008007703\n",
    "\n",
    "Topic  2\n",
    "protein  - P= 0.014365794\n",
    "proteomics  - P= 0.014036868\n",
    "damages  - P= 0.013797329\n",
    "dna  - P= 0.012998957\n",
    "possible  - P= 0.012180845\n",
    "proteins  - P= 0.0091615645\n",
    "modifications  - P= 0.0088425\n",
    "accumulation  - P= 0.007090772\n",
    "direct  - P= 0.007066036\n",
    "performed  - P= 0.006858151\n",
    "\n",
    "Topic  3\n",
    "strategies  - P= 0.017331606\n",
    "structural  - P= 0.016698044\n",
    "husbands  - P= 0.013327603\n",
    "work  - P= 0.009647215\n",
    "experiences  - P= 0.00907986\n",
    "approach  - P= 0.008296081\n",
    "gendered  - P= 0.007841238\n",
    "caregiving  - P= 0.0074790656\n",
    "approaches  - P= 0.0074426136\n",
    "deal  - P= 0.0071944036\n",
    "\n",
    "Topic  4\n",
    "structural  - P= 0.01409904\n",
    "strategies  - P= 0.011293924\n",
    "experiences  - P= 0.010676877\n",
    "husbands  - P= 0.009167888\n",
    "auditory  - P= 0.0071076383\n",
    "work  - P= 0.0066279923\n",
    "gender  - P= 0.005999501\n",
    "approaches  - P= 0.0057441127\n",
    "approach  - P= 0.00548574\n",
    "problems  - P= 0.005364729\n",
    "\n",
    "Topic  5\n",
    "auditory  - P= 0.028528903\n",
    "neurons  - P= 0.021563916\n",
    "individual  - P= 0.009536051\n",
    "tegmental  - P= 0.0089635495\n",
    "torus  - P= 0.008753919\n",
    "integration  - P= 0.008737798\n",
    "–  - P= 0.008698545\n",
    "patterns  - P= 0.008654427\n",
    "laminar  - P= 0.0086233895\n",
    "axons  - P= 0.008570297\n",
    "\n",
    "Topic  6\n",
    "age  - P= 0.002863048\n",
    "months  - P= 0.0028626046\n",
    "ice  - P= 0.002857396\n",
    "mice  - P= 0.002817289\n",
    "numbers  - P= 0.0028112691\n",
    "kda  - P= 0.0028109192\n",
    "alzheimer’s  - P= 0.0028053338\n",
    "disease  - P= 0.0027923447\n",
    "plaques  - P= 0.0027894997\n",
    "dna  - P= 0.0027821031\n",
    "\n",
    "Topic  7\n",
    "auditory  - P= 0.02685895\n",
    "neurons  - P= 0.020505337\n",
    "spinal  - P= 0.011076613\n",
    "targets  - P= 0.010763924\n",
    "neural  - P= 0.010539765\n",
    "audiomotor  - P= 0.010417759\n",
    "project  - P= 0.009710497\n",
    "–  - P= 0.009369334\n",
    "nucleus  - P= 0.009336974\n",
    "input  - P= 0.009303068\n",
    "\n",
    "Topic  8\n",
    "biological  - P= 0.024933385\n",
    "physical  - P= 0.019960593\n",
    "measures  - P= 0.019602107\n",
    "mental  - P= 0.017378531\n",
    "elderly  - P= 0.01371953\n",
    "data  - P= 0.012888425\n",
    "sample  - P= 0.012778336\n",
    "stressful  - P= 0.012095788\n",
    "health  - P= 0.012086122\n",
    "taiwanese  - P= 0.01155675\n",
    "\n",
    "Perplexity=  -7.472848060797519\n",
    "Coherence=  0.858348390841973\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing stopwords from the corpus and rerunning the LDA model, the resulting topics have become more coherent and interpretable. Let's analyze the main content of each topic:\n",
    "\n",
    "Topic 0:\n",
    "This topic appears to be related to gender roles and caregiving. Terms such as \"structural,\" \"strategies,\" \"husbands,\" and \"work\" suggest a focus on the experiences, approaches, and challenges faced by husbands in caregiving roles. The presence of terms like \"gender\" and \"caregiving\" reinforces the theme of gender dynamics in caregiving.\n",
    "\n",
    "Topic 1:\n",
    "This topic seems to revolve around proteomics and biological analysis. Terms like \"DNA,\" \"protein,\" \"proteomics,\" and \"modifications\" indicate a focus on studying proteins, their functions, and modifications. The inclusion of \"mental\" and \"experiences\" suggests a potential connection to mental health research within the context of proteomics.\n",
    "\n",
    "Topic 2:\n",
    "Similar to Topic 1, this topic is centered on proteins and proteomics. Terms such as \"protein,\" \"proteomics,\" \"damages,\" and \"DNA\" indicate a focus on the study of protein damage and accumulation. The terms \"possible\" and \"performed\" suggest research related to exploring possible connections and performing experiments in this domain.\n",
    "\n",
    "Topic 3:\n",
    "This topic overlaps with Topic 0, focusing on gender-related strategies and experiences. The terms \"strategies,\" \"structural,\" \"husbands,\" and \"work\" indicate a focus on gendered strategies and experiences in various settings. It appears to explore the approaches employed by husbands in different contexts, potentially related to caregiving or work environments.\n",
    "\n",
    "Topic 4:\n",
    "This topic combines terms related to structural and auditory aspects. Terms like \"structural,\" \"experiences,\" \"husbands,\" and \"work\" are similar to Topics 0 and 3, suggesting a connection to gendered experiences in work settings. The inclusion of \"auditory\" and \"problems\" indicates a potential link to auditory problems or challenges faced in relation to gender or work.\n",
    "\n",
    "Topic 5:\n",
    "This topic focuses on auditory and neural aspects. Terms such as \"auditory,\" \"neurons,\" \"tegmental,\" and \"integration\" suggest a focus on the auditory system and neural integration. It might explore the patterns, connections, and functions of auditory neurons and their integration within the neural network.\n",
    "\n",
    "Topic 6:\n",
    "This topic appears to be related to age and disease, particularly Alzheimer's disease. Terms like \"age,\" \"months,\" \"ice,\" \"mice,\" \"alzheimer's,\" and \"plaques\" indicate a focus on studying the effects of aging and ice exposure on mice, possibly in relation to Alzheimer's disease and the formation of plaques.\n",
    "\n",
    "Topic 7:\n",
    "This topic revolves around auditory and neural aspects, particularly targeting auditory neurons. Terms like \"auditory,\" \"neurons,\" \"spinal,\" \"targets,\" and \"nucleus\" suggest a focus on the neural pathways and targets associated with auditory processing. It might explore the input and connections between the auditory system and the neural network.\n",
    "\n",
    "Topic 8:\n",
    "This topic is centered on biological and physical measures in the context of mental health and stress. Terms like \"biological,\" \"physical,\" \"measures,\" \"mental,\" \"elderly,\" and \"stressful\" indicate a focus on studying the biological and physical indicators of mental health and stress, particularly among the elderly population. It might involve data collection and analysis related to stressful situations and their impact on mental and physical well-being.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In question 1) b), the results obtained without removing stopwords showed topics that were less coherent and harder to interpret. The topics had a mixture of words from different domains, leading to less meaningful and more scattered topics. After removing stopwords in the current results, the topics became more coherent and focused on specific themes.\n",
    "\n",
    "The coherence value in the current results (0.858) indicates a higher quality of topics compared to the initial results, where coherence was not provided. A higher coherence value suggests that the topics are more interpretable and have stronger semantic connections between the words within each topic.\n",
    "\n",
    "The perplexity value (-7.4728) obtained after removing stopwords is not directly comparable to the initial results since perplexity values depend on the specific dataset and the number of topics. However, a lower perplexity value generally indicates that the model has better learned the underlying patterns and structure of the data.\n",
    "\n",
    "The relationship between coherence, perplexity, and topic quality is indirect. Higher coherence suggests more interpretable and coherent topics, which are generally considered to be of higher quality. However, perplexity alone is not a strong indicator of topic quality. While a lower perplexity value can imply better modeling performance, it does not guarantee the semantic relevance or interpretability of the topics. Coherence is a more reliable metric for evaluating topic quality and human interpretability.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "texts = []\n",
    "file = open(\"TokenVieuxM.txt\", \"r\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    lt = line.split(\",\")\n",
    "    for i in range(len(lt)):\n",
    "        lt[i] = lt[i].replace('[','')\n",
    "        lt[i] = lt[i].replace(']','')\n",
    "        lt[i] = lt[i].replace('\"','')\n",
    "        lt[i] = lt[i].replace('\\n','')\n",
    "        lt[i] = lt[i].replace(' ', '')\n",
    "\n",
    "    ltc = [word for word in lt if not word in stopwords.words()]\n",
    "    texts.append(ltc)\n",
    "\n",
    "coherence_scores = []\n",
    "perplexity_scores = []\n",
    "topic_range = range(5, 21)\n",
    "\n",
    "for nb in topic_range:\n",
    "    id2word = Dictionary(texts)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    lda = LdaModel(corpus, num_topics=nb)\n",
    "\n",
    "    # Compute Perplexity\n",
    "    perplexity_lda = lda.log_perplexity(corpus)\n",
    "    perplexity_scores.append(perplexity_lda)\n",
    "\n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    coherence_scores.append(coherence_lda)\n",
    "\n",
    "    print(\"Number of Topics:\", nb)\n",
    "    print(\"Perplexity:\", perplexity_lda)\n",
    "    print(\"Coherence:\", coherence_lda)\n",
    "    print()\n",
    "\n",
    "# Plotting the coherence scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(topic_range, coherence_scores, marker='o')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.title(\"Coherence Scores for Different Numbers of Topics\")\n",
    "plt.xticks(topic_range)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
